{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxNxTY3wQKI_",
    "outputId": "c228cd29-28a9-47a6-e8d0-f5a2c68ad8b0"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSOKkL8yQa8J",
    "outputId": "fc9afbe9-625e-41c8-e88c-b33fa6ba561c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "07ca91063b724dee9ed9410d4af1dc8c",
      "bf6cf669e62d46fe9843677a57b17720",
      "8c20d721270d4529b4d091ab15045835",
      "b8d29304e4324a47b7a7a4fe0e4f8530",
      "502f21bdbc694b279d919787529c0c48",
      "145eb4e3a44b4b4da228438f7951b913",
      "1c06f83a3ee24379a2080a917f7a861a",
      "0064bb8beb9f46d1a8c18cb25d56d4a9",
      "1188d42a21614751b59b0fb79caf4f1f",
      "825ace18b7de492ebb73690d82f596d5",
      "4b6fafbe8f374e7bb8727e10f26feaa0",
      "e2755c970054458799e0997e6abbd8a0",
      "a75919c8174a4a888cb34a838ea50718",
      "f7cd7725388842c3b0b487438f4a3331",
      "b7909e9aa60d462dbea0fe0879f04e0d",
      "a3b161f561474b44a66e97eee69a5c60",
      "d5fc076471d54c9c881827a88062f51a",
      "eb815967ef4b4a61a5245b4ef056dab7",
      "add82784ac954d1ab3d655b7f5bd15b4",
      "399dfa01df734258a8889fc31aeef441",
      "058bd59976254190aacb6ea904d7035a",
      "5e7266c08f9c407583b608c22264fceb",
      "12ee358dc6934cf8a6193f0b5ba3e89a",
      "869ee9b2c7df438cb8280fb0cfaa9201",
      "a677626dbfc74d41bdadd4911a2e17a3",
      "cc511b38a2f143af84a25174cc0356c1",
      "13151688a375456896924fd9b002f8b7",
      "29ab4c8de46e4977893421ec3992d37a",
      "8cbc8d0e92d74663b9db298b889c5f7c",
      "6116159032d447e5bdef2f4636cc8a83",
      "60087ce6af5e408eb0faaee9a47f031d",
      "db8b9c41143b4f93a4c500fddd63f675",
      "6a32648cd4354c2bbabea1071cabeb00",
      "bfacfa8894034d01867824dcb9e5d9d1",
      "f8456b3847384189916e00b58d8a25da",
      "ee4167c8ef17403ca37a21593bd3e7b8",
      "76728c141da6487b86310174cab10604",
      "0698d39b1cf34032a89deb8702481663",
      "f635a02117614f37b25434c8f610ab41",
      "dad7bdd6706c487abb163be604f04446",
      "e9071c4e9da4437189a34c52c85a14a3",
      "5a33fff6e31b4c40b738e46240742b9f",
      "265c0b526a9948d4a5842eaa2cfcc69e",
      "557d2c66013746ca97177c0bc6d5a6e0",
      "bd0bba802098424fa22bc96957862fc6",
      "110eea2a6d3c44999c7b5cb5d17414e0",
      "f721f27fe4df470395d378966517e357",
      "33f121f341f0447987ead9fd3b25e8af",
      "0194cf2d03364dd78927090cc8429a4d",
      "239ce8e47e7947e68c36d96d297648eb",
      "179191b9d1b7473ab5ec802da38fa6bc",
      "9f0a33b43dbc4cec875307cbc128b680",
      "3722ead96b384b55affc521384588ab0",
      "09e22b5e75d442bab5d3f7c97bab03cd",
      "b51e5dd147794c5dac47074e8cecf58c",
      "242f038056704b1b8f2d787567552c76",
      "0f97ad8a751c4eebb0b023b4f6f24ceb",
      "b95ee672de624cefa63a9adbaaa460d3",
      "eb296609d4c944f79bc8bfd7e9b3114f",
      "9b99f376b79e4e399a9e42800bb2ca67",
      "a911a107c81448d29d1325ed00cdf11b",
      "a4cd9b316dca49be834504b73cb1eb67",
      "4042d2e8236c415dad2c27529e3fecd1",
      "0647ee8c7bae45919dc872cf8712fe81",
      "443e2b586a024ad6b570f9e2d459bcdc",
      "01995ababcf64ea79794b03e795f686c",
      "3e8ca08dc85e437d91abf7e95337dd03",
      "331a9e3ebc3b49c4a1579a09e1ac9d17",
      "e957b3d1fdc449bbb44cbb01a7f0916a",
      "e560e5637df64860a861ae62e0df014f",
      "41e4195525d8425d83b9c507b1d78ff7",
      "c766320bcdc643afbadc64852fd3433b",
      "88825e1e81e448158db24e315abca89c",
      "efbb5691ff1d4e4486abb40828f44deb",
      "7e67bb2263934aff98d7c84759ead9a2",
      "6c1dda71c3cd4c73bbc7c81e4f35e424",
      "0c3c7592753549a6a16cd7b0346bb0f2"
     ]
    },
    "id": "CxN8hPdpQa5I",
    "outputId": "d1ed15db-2a6a-435c-873f-93533eda497e"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"harouzie/vi_en-translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfkWAZbUQa1v"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rr37qsnfQayW"
   },
   "outputs": [],
   "source": [
    "src_lang = 'English'\n",
    "tgt_lang = 'Vietnamese'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {src_lang: None, tgt_lang: None}\n",
    "\n",
    "token_transform[src_lang] = get_tokenizer('basic_english')\n",
    "token_transform[tgt_lang] = get_tokenizer('basic_english')\n",
    "\n",
    "unk_id, pad_id, sos_id, eos_id = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "def yield_tokens(data_iter, lang):\n",
    "    for data in data_iter[lang]:\n",
    "        yield token_transform[lang](data)\n",
    "\n",
    "for lang in [src_lang, tgt_lang]:\n",
    "    train_iter = data['train']\n",
    "\n",
    "    vocab_transform[lang] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, lang),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True\n",
    "    )\n",
    "    vocab_transform[lang].set_default_index(unk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsRYwzmPQauk"
   },
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.tensor([sos_id] + token_ids + [eos_id])\n",
    "\n",
    "\n",
    "text_transform = {\n",
    "    lang: sequential_transforms(\n",
    "        token_transform[lang],\n",
    "        vocab_transform[lang],\n",
    "        tensor_transform\n",
    "    ) for lang in [src_lang, tgt_lang]\n",
    "}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for sample in batch:\n",
    "        src_sample, tgt_sample = sample[src_lang], sample[tgt_lang]\n",
    "        src_batch.append(text_transform[src_lang](src_sample).to(dtype=torch.int64))\n",
    "        tgt_batch.append(text_transform[tgt_lang](tgt_sample).to(dtype=torch.int64))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=pad_id, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=pad_id, batch_first=True)\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn4pyeVJQlOK"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    data['train'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    data['valid'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    data['test'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HN7vP_oQnZT"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "\n",
    "    embedding_dimension: int = 512\n",
    "    num_attention_heads: int = 8\n",
    "    attention_dropout_p: float = 0.0\n",
    "    hidden_dropout_p: float = 0.0\n",
    "    mlp_ratio: int = 4\n",
    "    encoder_depth: int = 3\n",
    "    decoder_depth: int = 3\n",
    "\n",
    "    src_vocab_size: int = len(vocab_transform[\"English\"])\n",
    "    tgt_vocab_size: int = len(vocab_transform[\"Vietnamese\"])\n",
    "\n",
    "    max_src_len: int = 512\n",
    "    max_tgt_len: int = 512\n",
    "    learn_pos_embed: bool = False\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, max_len, embed_dim, requires_grad=False):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.encodings = self._build_positional_encodings()\n",
    "\n",
    "    def _build_positional_encodings(self):\n",
    "\n",
    "        encoding = torch.zeros(self.max_len, self.embed_dim, dtype=torch.float)\n",
    "        postion_idx = torch.arange(0, self.max_len, dtype=torch.float).reshape(-1,1)\n",
    "        embed_dim_skip_idx = torch.arange(0, self.embed_dim, step=2, dtype=torch.float)\n",
    "\n",
    "        encoding[:, 0::2] = torch.sin(postion_idx / (10000 ** (embed_dim_skip_idx / self.embed_dim)))\n",
    "        encoding[:, 1::2] = torch.cos(postion_idx / (10000 ** (embed_dim_skip_idx / self.embed_dim)))\n",
    "\n",
    "        encoding = nn.Parameter(encoding, requires_grad=self.requires_grad)\n",
    "\n",
    "        return encoding\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "\n",
    "        encodings = self.encodings[:seq_len]\n",
    "\n",
    "        x = x + encodings\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    All the embeddings we need for the source and target langauge. Both source and target need:\n",
    "\n",
    "    - Token Embeddings\n",
    "    - Positional Embedings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.src_embeddings = nn.Embedding(config.src_vocab_size, config.embedding_dimension)\n",
    "        self.tgt_embeddings = nn.Embedding(config.tgt_vocab_size, config.embedding_dimension)\n",
    "\n",
    "        self.src_positional_encodings = PositionalEncoding(config.max_src_len,\n",
    "                                                           config.embedding_dimension,\n",
    "                                                           config.learn_pos_embed)\n",
    "        self.tgt_positional_encodings = PositionalEncoding(config.max_tgt_len,\n",
    "                                                           config.embedding_dimension,\n",
    "                                                           config.learn_pos_embed)\n",
    "\n",
    "    def forward_src(self, input_ids):\n",
    "        embeddings = self.src_embeddings(input_ids)\n",
    "        embeddings = self.src_positional_encodings(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def forward_tgt(self, input_ids):\n",
    "        embeddings = self.tgt_embeddings(input_ids)\n",
    "        embeddings = self.tgt_positional_encodings(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Regular Self-Attention but in this case we utilize flash_attention\n",
    "    incorporated in the F.scaled_dot_product_attention to speed up our training.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        assert config.embedding_dimension % config.num_attention_heads == 0, \"Double check embedding dim divisible by number of heads\"\n",
    "\n",
    "\n",
    "        self.head_dim = config.embedding_dimension // config.num_attention_heads\n",
    "\n",
    "\n",
    "        self.q_proj = nn.Linear(config.embedding_dimension, config.embedding_dimension)\n",
    "        self.k_proj = nn.Linear(config.embedding_dimension, config.embedding_dimension)\n",
    "        self.v_proj = nn.Linear(config.embedding_dimension, config.embedding_dimension)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.embedding_dimension, config.embedding_dimension)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                src,\n",
    "                tgt=None,\n",
    "                attention_mask=None,\n",
    "                causal=False):\n",
    "\n",
    "\n",
    "        batch, src_len, embed_dim = src.shape\n",
    "\n",
    "\n",
    "        if tgt is None:\n",
    "            q = self.q_proj(src).reshape(batch, src_len, self.config.num_attention_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "            k = self.k_proj(src).reshape(batch, src_len, self.config.num_attention_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "            v = self.v_proj(src).reshape(batch, src_len, self.config.num_attention_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "\n",
    "            if attention_mask is not None:\n",
    "\n",
    "                attention_mask = attention_mask.bool()\n",
    "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,src_len,1)\n",
    "\n",
    "            attention_out = F.scaled_dot_product_attention(q,k,v,\n",
    "                                                           attn_mask=attention_mask,\n",
    "                                                           dropout_p=self.config.attention_dropout_p if self.training else 0.0,\n",
    "                                                           is_causal=causal)\n",
    "\n",
    "\n",
    "        else:\n",
    "            tgt_len = tgt.shape[1]\n",
    "\n",
    "            q = self.q_proj(tgt).reshape(batch, tgt_len, self.config.num_attention_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "            k = self.k_proj(src).reshape(batch, src_len, self.config.num_attention_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "            v = self.v_proj(src).reshape(batch, src_len, self.config.num_attention_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "\n",
    "            if attention_mask is not None:\n",
    "\n",
    "                attention_mask = attention_mask.bool()\n",
    "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,tgt_len,1)\n",
    "\n",
    "            attention_out = F.scaled_dot_product_attention(q,k,v,\n",
    "                                                           attn_mask=attention_mask,\n",
    "                                                           dropout_p=self.config.attention_dropout_p if self.training else 0.0,\n",
    "                                                           is_causal=False)\n",
    "\n",
    "\n",
    "        attention_out = attention_out.transpose(1,2).flatten(2)\n",
    "        attention_out = self.out_proj(attention_out)\n",
    "\n",
    "        return attention_out\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Regular MLP module after our attention computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_size = config.embedding_dimension * config.mlp_ratio\n",
    "        self.intermediate_dense = nn.Linear(config.embedding_dimension, hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.intermediate_dropout = nn.Dropout(config.hidden_dropout_p)\n",
    "\n",
    "        self.output_dense = nn.Linear(hidden_size, config.embedding_dimension)\n",
    "        self.output_dropout = nn.Dropout(config.hidden_dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intermediate_dense(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.intermediate_dropout(x)\n",
    "\n",
    "        x = self.output_dense(x)\n",
    "        x = self.output_dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Stacks together a Self-Attention module and MLP Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_attention = Attention(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_p)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dimension)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(config.embedding_dimension)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "\n",
    "        x = x + self.dropout(self.enc_attention(x, attention_mask=attention_mask))\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Stacks together a Causal-Attention of our target language, Cross Attention with encoded source language,\n",
    "    and a MLP layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_attention = Attention(config)\n",
    "        self.dec_attention_dropout = nn.Dropout(config.hidden_dropout_p)\n",
    "        self.dec_attention_layernorm = nn.LayerNorm(config.embedding_dimension)\n",
    "\n",
    "        self.cross_attention = Attention(config)\n",
    "        self.cross_attention_dropout = nn.Dropout(config.hidden_dropout_p)\n",
    "        self.cross_attention_layernorm = nn.LayerNorm(config.embedding_dimension)\n",
    "\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(config.embedding_dimension)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        tgt = tgt + self.dec_attention_dropout(self.dec_attention(tgt, attention_mask=tgt_mask, causal=True))\n",
    "        tgt = self.dec_attention_layernorm(tgt)\n",
    "\n",
    "        tgt = tgt + self.cross_attention_dropout(self.cross_attention(src, tgt, attention_mask=src_mask))\n",
    "        tgt = self.cross_attention_layernorm(tgt)\n",
    "\n",
    "        tgt = tgt + self.feed_forward(tgt)\n",
    "        tgt = self.final_layer_norm(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.encodings = Embeddings(config)\n",
    "\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(config) for _ in range(config.encoder_depth)]\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [TransformerDecoderLayer(config) for _ in range(config.decoder_depth)]\n",
    "        )\n",
    "\n",
    "        self.head = nn.Linear(config.embedding_dimension, config.tgt_vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                src_ids,\n",
    "                tgt_ids,\n",
    "                src_attention_mask=None,\n",
    "                tgt_attention_mask=None):\n",
    "\n",
    "        src_embeddings = self.encodings.forward_src(src_ids)\n",
    "        tgt_embeddings = self.encodings.forward_tgt(tgt_ids)\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings,\n",
    "                                   src_attention_mask)\n",
    "\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(src_embeddings,\n",
    "                                   tgt_embeddings,\n",
    "                                   src_attention_mask,\n",
    "                                   tgt_attention_mask)\n",
    "\n",
    "        pred = self.head(tgt_embeddings)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def inference(self,\n",
    "                  src_ids,\n",
    "                  tgt_start_id=2,\n",
    "                  tgt_end_id=3,\n",
    "                  max_len=512):\n",
    "\n",
    "        tgt_ids = torch.tensor([tgt_start_id], device=src_ids.device).reshape(1,1)\n",
    "\n",
    "\n",
    "        src_embeddings = self.encodings.forward_src(src_ids)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings)\n",
    "\n",
    "\n",
    "        for i in range(max_len):\n",
    "\n",
    "            tgt_embeddings = self.encodings.forward_tgt(tgt_ids)\n",
    "            for layer in self.decoder:\n",
    "                tgt_embeddings = layer(src_embeddings,\n",
    "                                       tgt_embeddings)\n",
    "\n",
    "\n",
    "            tgt_embeddings = tgt_embeddings[:, -1]\n",
    "\n",
    "\n",
    "            pred = self.head(tgt_embeddings)\n",
    "            pred = pred.argmax(axis=-1).unsqueeze(0)\n",
    "            tgt_ids = torch.cat([tgt_ids,pred], axis=-1)\n",
    "\n",
    "            if torch.all(pred == tgt_end_id):\n",
    "                break\n",
    "\n",
    "        return tgt_ids.squeeze().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHUupGsxQnQs"
   },
   "outputs": [],
   "source": [
    "config = TransformerConfig()\n",
    "\n",
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZG9BXkabQrf6",
    "outputId": "1ba55563-ca1e-43c7-8b67-16068ed1d1ee"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "checkpoint = torch.load(\"/content/model_after.pth\", map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxuegiIUQtAz",
    "outputId": "7723d047-681d-471e-e277-5c110b501ba4"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68ERKpDhQvDP",
    "outputId": "e913e8c8-278f-48e6-a018-9c39cf795fcf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "file_path = \"/content/Dataset_Final.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "\n",
    "column_name = \"English\"\n",
    "\n",
    "\n",
    "if df[column_name].dtype == object:\n",
    "    print(\"Dữ liệu là văn bản -> Dùng TF-IDF để tạo vector\")\n",
    "\n",
    "    text_data = df[column_name].astype(str).tolist()\n",
    "    vectorizer = TfidfVectorizer(max_features=300)\n",
    "    vectors = vectorizer.fit_transform(text_data).toarray().astype(np.float32)\n",
    "\n",
    "else:\n",
    "    print(\"Dữ liệu là số -> Dùng trực tiếp\")\n",
    "    vectors = df.select_dtypes(include=[np.number]).to_numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "vector_dim = vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(vector_dim)\n",
    "index.add(vectors)\n",
    "\n",
    "print(f\"Đã tạo FAISS index với {index.ntotal} vector, chiều: {vector_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGHtKJHvRLnl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def translate_with_reference(input_text, model, df, text_transform, vocab_transform, config, device):\n",
    "    src_lang = \"English\"\n",
    "    tgt_lang = \"Vietnamese\"\n",
    "    pad_id = 0\n",
    "    sos_id = 2\n",
    "    eos_id = 3\n",
    "\n",
    "\n",
    "    input_tensor = text_transform[src_lang](input_text).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_embedding = model.encodings.forward_src(input_tensor)\n",
    "\n",
    "\n",
    "    english_sentences = df[\"English\"].tolist()\n",
    "    ref_tensors = [text_transform[src_lang](sent) for sent in english_sentences]\n",
    "    ref_batch = pad_sequence(ref_tensors, padding_value=pad_id, batch_first=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_embeddings = model.encodings.forward_src(ref_batch)\n",
    "\n",
    "\n",
    "    src_embedding_mean = src_embedding.mean(dim=1)\n",
    "    ref_embeddings_mean = ref_embeddings.mean(dim=1)\n",
    "    similarities = F.cosine_similarity(src_embedding_mean, ref_embeddings_mean, dim=1)\n",
    "    ref_idx = similarities.argmax().item()\n",
    "    ref_sentence_en = english_sentences[ref_idx]\n",
    "\n",
    "\n",
    "    ref_tensor = text_transform[src_lang](ref_sentence_en).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        ref_embedding = model.encodings.forward_src(ref_tensor)\n",
    "\n",
    "\n",
    "    combined_embedding = torch.cat([ref_embedding, src_embedding], dim=1)\n",
    "    combined_embedding = combined_embedding[:, :config.max_src_len, :]\n",
    "\n",
    "\n",
    "    ref_len = ref_embedding.shape[1]\n",
    "    src_len = src_embedding.shape[1]\n",
    "    combined_len = min(ref_len + src_len, config.max_src_len)\n",
    "    src_attention_mask = torch.ones(1, combined_len, dtype=torch.int64).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for layer in model.encoder:\n",
    "            combined_embedding = layer(combined_embedding, attention_mask=src_attention_mask)\n",
    "\n",
    "\n",
    "    src_embedding_processed = combined_embedding[:, ref_len:, :]\n",
    "\n",
    "\n",
    "    def inference_with_embedding(model, src_embedding, tgt_start_id=sos_id, tgt_end_id=eos_id, max_len=512):\n",
    "        tgt_ids = torch.tensor([tgt_start_id], device=src_embedding.device).reshape(1, 1)\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_len):\n",
    "                tgt_embeddings = model.encodings.forward_tgt(tgt_ids)\n",
    "                for layer in model.decoder:\n",
    "                    tgt_embeddings = layer(src_embedding, tgt_embeddings)\n",
    "                tgt_embeddings = tgt_embeddings[:, -1]\n",
    "                pred = model.head(tgt_embeddings)\n",
    "                pred = pred.argmax(dim=-1).unsqueeze(0)\n",
    "                tgt_ids = torch.cat([tgt_ids, pred], dim=-1)\n",
    "                if torch.all(pred == tgt_end_id):\n",
    "                    break\n",
    "        return tgt_ids.squeeze().cpu().tolist()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    tgt_ids = inference_with_embedding(model, src_embedding_processed)\n",
    "\n",
    "\n",
    "    vocab = vocab_transform[tgt_lang]\n",
    "    translated_sentence = \" \".join([vocab.lookup_token(id) for id in tgt_ids if id not in [pad_id, sos_id, eos_id]])\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uCeudN2RPN-"
   },
   "outputs": [],
   "source": [
    "def translate_without_reference(input_text, model, text_transform, vocab_transform, config, device):\n",
    "\n",
    "    src_lang = \"English\"\n",
    "    tgt_lang = \"Vietnamese\"\n",
    "    pad_id = 0\n",
    "    sos_id = 2\n",
    "    eos_id = 3\n",
    "\n",
    "\n",
    "    input_tensor = text_transform[src_lang](input_text).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_embedding = model.encodings.forward_src(input_tensor)\n",
    "        for layer in model.encoder:\n",
    "            src_embedding = layer(src_embedding)\n",
    "\n",
    "\n",
    "    def inference_without_rag(src_embedding, tgt_start_id=sos_id, tgt_end_id=eos_id, max_len=512):\n",
    "        tgt_ids = torch.tensor([tgt_start_id], device=src_embedding.device).reshape(1, 1)\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_len):\n",
    "                tgt_embeddings = model.encodings.forward_tgt(tgt_ids)\n",
    "                for layer in model.decoder:\n",
    "                    tgt_embeddings = layer(src_embedding, tgt_embeddings)\n",
    "                tgt_embeddings = tgt_embeddings[:, -1]\n",
    "                pred = model.head(tgt_embeddings)\n",
    "                pred = pred.argmax(dim=-1).unsqueeze(0)\n",
    "                tgt_ids = torch.cat([tgt_ids, pred], dim=-1)\n",
    "                if torch.all(pred == tgt_end_id):\n",
    "                    break\n",
    "        return tgt_ids.squeeze().cpu().tolist()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    tgt_ids = inference_without_rag(src_embedding)\n",
    "\n",
    "\n",
    "    vocab = vocab_transform[tgt_lang]\n",
    "    translated_sentence = \" \".join([vocab.lookup_token(id) for id in tgt_ids if id not in [pad_id, sos_id, eos_id]])\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5COFFofnRQ3W",
    "outputId": "4eb9056f-b26d-4d12-f549-0805649dbc33"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_text = \"Anxiety disorders can interfere with daily life and cause severe mental health issues.\"\n",
    "\n",
    "df = df.dropna(subset=[\"English\"])\n",
    "df[\"English\"] = df[\"English\"].astype(str)\n",
    "\n",
    "translated_text_with_rag = translate_with_reference(input_text, model, df, text_transform, vocab_transform, config, device)\n",
    "print(f\"Bản dịch với RAG: {translated_text_with_rag}\")\n",
    "\n",
    "translated_text_without_rag = translate_without_reference(input_text, model, text_transform, vocab_transform, config, device)\n",
    "print(f\"Bản dịch không RAG: {translated_text_without_rag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nulc5JP-XxwL",
    "outputId": "70dfdcfc-708d-46d1-987e-4fa45b4939af"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Hepatitis B virus infection\",\n",
    "    \"Doctors are concerned about Other disorders of nervous system in diseases classified elsewhere in the community\",\n",
    "    \"Dengue hemorrhagic fever needs careful monitoring to prevent complications.\",\n",
    "    \"Chronic kidney disease requires long-term treatment.\",\n",
    "    \"Asthma is a chronic disease that affects the airways and causes breathing difficulties.\",\n",
    "    \"Anxiety disorders can interfere with daily life and cause severe mental health issues.\"\n",
    "]\n",
    "\n",
    "\n",
    "df = df.dropna(subset=[\"English\"])\n",
    "df[\"English\"] = df[\"English\"].astype(str)\n",
    "\n",
    "\n",
    "for input_text in sentences:\n",
    "\n",
    "    translated_text_with_rag = translate_with_reference(input_text, model, df, text_transform, vocab_transform, config, device)\n",
    "    print(f\"Câu gốc: {input_text}\")\n",
    "    print(f\"Bản dịch với RAG: {translated_text_with_rag}\")\n",
    "\n",
    "\n",
    "    translated_text_without_rag = translate_without_reference(input_text, model, text_transform, vocab_transform, config, device)\n",
    "    print(f\"Bản dịch không RAG: {translated_text_without_rag}\")\n",
    "\n",
    "\n",
    "    print(\"__________________________\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
